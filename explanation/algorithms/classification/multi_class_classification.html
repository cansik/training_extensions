
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Multi-class Classification &#8212; OpenVINO Training Extensions  documentation</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Multi-label Classification" href="multi_label_classification.html" />
    <link rel="prev" title="Classification" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/otx-logo-black-mini.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../index.html">
   Algorithms
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../anomaly/index.html">
     Anomaly Recognition
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../anomaly/anomaly_classification.html">
       Anomaly Classificaiton
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../anomaly/anomaly_detection.html">
       Anomaly Detection
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../anomaly/anomaly_segmentation.html">
       Anomaly Segmentation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../action/index.html">
     Action Recognition
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../action/action_classification.html">
       Action Recognition
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../action/action_detection.html">
       Action Detection
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     Classification
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Multi-class Classification
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="multi_label_classification.html">
       Multi-label Classification
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hierarhical_classification.html">
       Hierarchical Classification
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../segmentation/index.html">
     Segmentation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../segmentation/semantic_segmentation.html">
       Semantic Segmentation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../segmentation/instance_segmentation.html">
       Instance Segmentation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../object_detection/index.html">
     Object Detection
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../object_detection/object_detection.html">
       Object Detection
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../object_detection/rotated_object_detection.html">
       Object Detection With Rotated Bounding Boxes
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../additional_features/index.html">
   Additional Features
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../additional_features/models_optimization.html">
     Models Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../additional_features/hpo.html">
     Hyperparameters Oprimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../additional_features/auto_configuration.html">
     Auto-configuration
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-format">
   Dataset Format
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#models">
   Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semi-supervised-learning">
   Semi-supervised Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-supervised-learning">
   Self-supervised Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#incremental-learning">
   Incremental Learning
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="multi-class-classification">
<h1>Multi-class Classification<a class="headerlink" href="#multi-class-classification" title="Permalink to this headline">¶</a></h1>
<p>Multi-class classification is the problem of classifying instances into one of two or more classes. We solve this problem in a common fashion, based on the feature extractor backbone and classifier head that predicts the distribution probability of the categories from the given corpus.
For the supervised training we use the following algorithms components:</p>
<ul id="mcl-cls-supervised-pipeline">
<li><p><code class="docutils literal notranslate"><span class="pre">Augmentations</span></code>: Besides basic augmentations like random flip and random rotate, we use <a class="reference external" href="https://arxiv.org/abs/1912.02781">Augmix</a>. This advanced type of augmentations helps to significantly expand the training distribution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>: <a class="reference external" href="https://arxiv.org/abs/2209.06585">Sharpness Aware Minimization (SAM)</a>. Wrapper upon the <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> optimizer that helps to achieve better generalization minimizing simultaneously loss value and loss sharpness.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Learning</span> <span class="pre">rate</span> <span class="pre">schedule</span></code>: <a class="reference external" href="https://arxiv.org/abs/1608.03983v5">Cosine Annealing</a>. It is a common learning rate scheduler that tends to work well on average for this task on a variety of different datasets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Loss</span> <span class="pre">function</span></code>: We use standart <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">Cross Entropy Loss</a>  to train a model. However, for the class-incremental scenario we use <a class="reference external" href="https://arxiv.org/abs/2110.02444">Influence-Balanced Loss</a>. IB loss is a solution for class-imbalance, which avoids overfitting to the majority classes re-weighting the influential samples.</p></li>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">Training</span> <span class="pre">technique</span></code></dt><dd><ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1812.01187">No Bias Decay (NBD)</a>: To add adaptability to the training pipeline and prevent overfitting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Early</span> <span class="pre">stopping</span></code>: To add adaptability to the training pipeline and prevent overfitting. You can use early stopping like the below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train {TEMPLATE} ... \
    params \
    --learning_parameters.enable_early_stopping=True \      # is early stopping used
    --learning_parameters.early_stop_start=3 \              # the number of epochs (iters) in which early stopping proceeds
    --learning_parameters.early_stop_patience=8 \           # (for epoch runner) stop if the model don&#39;t improve within the number of epochs of patience
    --learning_parameters.early_stop_iteration_patience=8 \ # (for iter runner) stop if the model don&#39;t improve within the number of iterations of patience
</pre></div>
</div>
</li>
<li><p><a class="reference external" href="https://github.dev/openvinotoolkit/training_extensions/blob/develop/otx/mpa/modules/datasets/samplers/balanced_sampler.py#L11">Balanced Sampler</a>: To create an efficient batch that consists of balanced samples over classes, reducing the iteration size as well.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning (SupCon)</a>: To enhance the performance of the algorithm in case when we have a small number of data. More specifically, we train a model with two heads: classification head with Influence-Balanced Loss and contrastive head with <a class="reference external" href="https://arxiv.org/abs/2103.03230">Barlow Twins loss</a>. It enables using <cite>–learning_parameters.enable_supcon=True</cite> in CLI.
The below table shows how much performance SupCon improved compared with baseline performance on three baseline datasets with 10 samples per class: CIFAR10, Eurosat-10, and Food-101.</p>
<table class="table">
<colgroup>
<col style="width: 28%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 15%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>CIFAR10</p></th>
<th class="head"></th>
<th class="head"><p>Eurosat-10</p></th>
<th class="head"></th>
<th class="head"><p>Food-101</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>SupCon</p></td>
<td><p>SL</p></td>
<td><p>SupCon</p></td>
<td><p>SL</p></td>
<td><p>SupCon</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet-V3-large-1x</p></td>
<td><p>55.06</p></td>
<td><p>58.88</p></td>
<td><p>77.60</p></td>
<td><p>78.70</p></td>
<td><p>34.83</p></td>
<td><p>34.38</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B0</p></td>
<td><p>42.81</p></td>
<td><p>46.35</p></td>
<td><p>66.87</p></td>
<td><p>70.23</p></td>
<td><p>37.26</p></td>
<td><p>39.17</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-V2-S</p></td>
<td><p>59.78</p></td>
<td><p>63.13</p></td>
<td><p>81.84</p></td>
<td><p>83.12</p></td>
<td><p>51.32</p></td>
<td><p>54.84</p></td>
</tr>
</tbody>
</table>
<p>You can use SupCon training like the below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train {TEMPLATE} ... \
    params \
    --learning_parameters.enable_supcon=True
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="section" id="dataset-format">
<h2>Dataset Format<a class="headerlink" href="#dataset-format" title="Permalink to this headline">¶</a></h2>
<p>We support a commonly used format for multi-class image classification task: <a class="reference external" href="https://www.image-net.org/">imagenet</a> class folder format.
This format has the following structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>data
├── train
    ├── class 0
        ├── 0.png
        ├── 1.png
        ...
        └── N.png
    ├── class 1
        ├── 0.png
        ├── 1.png
        ...
        └── N.png
    ...
    └── class N
        ├── 0.png
        ├── 1.png
        ...
        └── N.png
└── val
    ...
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please, refer to our <a class="reference internal" href="../../../tutorials/base/how_to_train/classification.html"><span class="doc">dedicated tutorial</span></a> for more information how to train, validate and optimize classificaiton models.</p>
</div>
</div>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p id="classificaiton-models">We support the following ready-to-use model templates:</p>
<table class="table">
<colgroup>
<col style="width: 77%" />
<col style="width: 9%" />
<col style="width: 8%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/otx/algorithms/classification/configs/mobilenet_v3_large_1_cls_incr/template.yaml">Custom_Image_Classification_MobileNet-V3-large-1x</a></p></td>
<td><p>MobileNet-V3-large-1x</p></td>
<td><p>0.44</p></td>
<td><p>4.29</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/otx/algorithms/classification/configs/efficientnet_b0_cls_incr/template.yaml">Custom_Image_Classification_EfficinetNet-B0</a></p></td>
<td><p>EfficientNet-B0</p></td>
<td><p>0.81</p></td>
<td><p>4.09</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/otx/algorithms/classification/configs/efficientnet_v2_s_cls_incr/template.yaml">Custom_Image_Classification_EfficientNet-V2-S</a></p></td>
<td><p>EfficientNet-V2-S</p></td>
<td><p>5.76</p></td>
<td><p>20.23</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://arxiv.org/abs/2104.00298">EfficientNet-V2-S</a> has more parameters and Flops and needs more time to train, meanwhile providing superior classification performance. <a class="reference external" href="https://arxiv.org/abs/1905.02244">MobileNet-V3-large-1x</a> is the best choice when training time and computational cost are in priority, nevertheless, this template provides competitive accuracy as well.
<a class="reference external" href="https://arxiv.org/abs/1905.11946">EfficientNet-B0</a> consumes more Flops compared to MobileNet, providing better performance on large datasets, but may be not so stable in case of a small amount of training data.</p>
<p>Besides this, we support public backbones from <a class="reference external" href="https://pytorch.org/vision/stable/index.html">torchvision</a>, <a class="reference external" href="https://github.com/osmr/imgclsmob">pytorchcv</a>, <a class="reference external" href="https://github.com/open-mmlab/mmclassification">mmcls</a> and <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo">OpenVino Model Zoo</a>.
Please, refer to the <a class="reference internal" href="../../../tutorials/advanced/backbones.html"><span class="doc">tutorial</span></a> how to customize models and run public backbones.</p>
<p>To see which public backbones are available for the task, the following command can be executed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx find --backbone {torchvision, pytorchcv, mmcls, omz.mmcls}
</pre></div>
</div>
<p>In the table below the top-1 accuracy on some academic datasets using our <a class="reference internal" href="#mcl-cls-supervised-pipeline"><span class="std std-ref">supervised pipeline</span></a> is presented. The results were obtained on our templates without any changes. We use 224x224 image resolution, for other hyperparameters, please, refer to the related template. We trained each model with single Nvidia GeForce RTX3090.</p>
<table class="table">
<colgroup>
<col style="width: 27%" />
<col style="width: 20%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>CIFAR100</p></th>
<th class="head"><p>cars</p></th>
<th class="head"><p>flowers</p></th>
<th class="head"><p>pets</p></th>
<th class="head"><p>SVHN</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MobileNet-V3-large-1x</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-B0</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-V2-S</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="semi-supervised-learning">
<h2>Semi-supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Semi-SL (Semi supervised Learning) is a type of machine learning algorithm that uses both labeled and unlabeled data to improve the performance of the model. This is particularly useful when labeled data is limited, expensive or time-consuming to obtain.</p>
<p><a class="reference external" href="https://arxiv.org/abs/2001.07685">FixMatch</a> is a specific implementation of Semi-SL that has been shown to be effective in various applications. FixMatch introduces pseudo-labeling, which is the process of generating labels for the unlabeled data and treating them as if they were labeled data. Pseudo-labeling is based on the idea that the model’s prediction for the unlabeled data is likely to be correct, which can improve the model’s accuracy and reduce the need for labeled data.</p>
<p>In Semi-SL, the pseudo-labeling process is combined with a consistency loss that ensures that the predictions of the model are consistent across augmented versions of the same data. This helps to reduce the impact of noisy or incorrect labels that may arise from the pseudo-labeling process. Additionally, Our algorithm uses a combination of strong data augmentations and a specific optimizer called Sharpness-Aware Minimization (SAM) to further improve the accuracy of the model.</p>
<p>Overall, OTX Semi-SL are powerful techniques for improving the performance of machine learning models with limited labeled data. They can be particularly useful in domains where labeled data is expensive or difficult to obtain, and can help to reduce the time and cost associated with collecting labeled data.</p>
<ul class="simple" id="mcl-cls-semi-supervised-pipeline">
<li><p><code class="docutils literal notranslate"><span class="pre">Pseudo-labeling</span> <span class="pre">(FixMatch)</span></code>: A specific implementation of Semi-SL that combines the use of pseudo-labeling with a consistency loss, strong data augmentations, and a specific optimizer called Sharpness-Aware Minimization (SAM) to improve the performance of the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adaptable</span> <span class="pre">Threshold</span></code>: A novel addition to our solution that calculates a class-wise threshold for pseudo-labeling, which can solve the issue of imbalanced data and produce high-quality pseudo-labels that improve the overall score.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Unlabeled</span> <span class="pre">Warm</span> <span class="pre">Up</span> <span class="pre">Loss</span></code>: A technique for preventing the initial unstable learning of pseudo-labeling by increasing the coefficient of the unlabeled loss from 0 to 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Exponential</span> <span class="pre">Moving</span> <span class="pre">Average</span> <span class="pre">(EMA)</span></code>: A technique for maintaining a moving average of the model’s parameters, which can improve the generalization performance of the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Other</span> <span class="pre">solutions</span></code>: Other than that, we use several solutions that apply to supervised learning (No bias Decay, Augmentations, etc.).</p></li>
</ul>
<p>Please, refer to the <a class="reference internal" href="../../../tutorials/advanced/semi_sl.html"><span class="doc">tutorial</span></a> how to train semi supervised learning. Based on MobileNet-V3-large-1x, it takes about 3 times longer than conventional supervised learning.</p>
<p>In the table below the top-1 accuracy on some academic datasets using our pipeline is presented. Same as the supervised setting except for an image for unlabeled and an additional batch size.</p>
<p>4 images per class (+ unlabeled image for Semi-SL)</p>
<table class="table">
<colgroup>
<col style="width: 31%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>CIFAR10</p></th>
<th class="head"></th>
<th class="head"><p>SVHN</p></th>
<th class="head"></th>
<th class="head"><p>FMNIST</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet-V3-large-1x</p></td>
<td><p>40.75</p></td>
<td><p>43.13</p></td>
<td><p>23.32</p></td>
<td><p>27.85</p></td>
<td><p>68.2</p></td>
<td><p>71.84</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B0</p></td>
<td><p>42.24</p></td>
<td><p>44.23</p></td>
<td><p>28.09</p></td>
<td><p>32.96</p></td>
<td><p>68.58</p></td>
<td><p>70.79</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-V2-S</p></td>
<td><p>36.03</p></td>
<td><p>39.66</p></td>
<td><p>16.81</p></td>
<td><p>20.28</p></td>
<td><p>65.99</p></td>
<td><p>69.61</p></td>
</tr>
</tbody>
</table>
<p>10 images per class (+ unlabeled image for Semi-SL)</p>
<table class="table">
<colgroup>
<col style="width: 31%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>CIFAR10</p></th>
<th class="head"></th>
<th class="head"><p>SVHN</p></th>
<th class="head"></th>
<th class="head"><p>FMNIST</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet-V3-large-1x</p></td>
<td><p>50.77</p></td>
<td><p>52.16</p></td>
<td><p>38.73</p></td>
<td><p>48.36</p></td>
<td><p>73.33</p></td>
<td><p>77.04</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet-B0</p></td>
<td><p>52.69</p></td>
<td><p>58.35</p></td>
<td><p>46.04</p></td>
<td><p>61.79</p></td>
<td><p>74.56</p></td>
<td><p>80.14</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet-V2-S</p></td>
<td><p>48.84</p></td>
<td><p>55</p></td>
<td><p>26.16</p></td>
<td><p>47.99</p></td>
<td><p>74.6</p></td>
<td><p>80.92</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This result can vary greatly depending on the image selected for each class. Also, since there are few labeled settings for the Semi-SL algorithm, Some models may require larger datasets for better results.</p>
</div>
</div>
<div class="section" id="self-supervised-learning">
<h2>Self-supervised Learning<a class="headerlink" href="#self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Self-supervised learning can be one of the solutions if the user has a small data set, but label information is not yet available.
General self-supervised Learning in academia is commonly used to obtain well-pretrained weights from a source dataset without label information.
However, in real-world industries, it is difficult to apply because of small datasets, limited resources, or training in minutes.</p>
<p>For these cases, OTX provides improved self-supervised learning recipes that can be applied to the above harsh environments.
We adapted <a class="reference external" href="https://arxiv.org/abs/2006.07733">BYOL</a> as our self-supervised method.
Users only need a few more minutes to use these self-supervised learning recipes and can expect improved performance, especially in low-data regimes.</p>
<p>Below is graphs of performance improvement for three baseline datasets: CIFAR10, CIFAR100, and Food-101.
The graphs below show how much performance improvement over baseline was achieved using our self-supervised learning recipes.
In particular, the smaller the data, the greater the performance improvement can be expected.</p>
<a class="reference internal image-reference" href="../../../_images/multi_cls_selfsl_performance_CIFAR10.png"><img alt="../../../_images/multi_cls_selfsl_performance_CIFAR10.png" src="../../../_images/multi_cls_selfsl_performance_CIFAR10.png" style="width: 600px;" /></a>
<a class="reference internal image-reference" href="../../../_images/multi_cls_selfsl_performance_CIFAR100.png"><img alt="../../../_images/multi_cls_selfsl_performance_CIFAR100.png" src="../../../_images/multi_cls_selfsl_performance_CIFAR100.png" style="width: 600px;" /></a>
<a class="reference internal image-reference" href="../../../_images/multi_cls_selfsl_performance_Food-101.png"><img alt="../../../_images/multi_cls_selfsl_performance_Food-101.png" src="../../../_images/multi_cls_selfsl_performance_Food-101.png" style="width: 600px;" /></a>
<p>You can use Self-supervised learning like the below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train {TEMPLATE} ... \
    params \
    --algo_backend.train_type=SELFSUPERVISED
</pre></div>
</div>
</div>
<div class="section" id="incremental-learning">
<h2>Incremental Learning<a class="headerlink" href="#incremental-learning" title="Permalink to this headline">¶</a></h2>
<p>To be added soon</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">Classification</a>
    <a class='right-next' id="next-link" href="multi_label_classification.html" title="next page">Multi-label Classification</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, OpenVINO Training Extensions Contributors.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>