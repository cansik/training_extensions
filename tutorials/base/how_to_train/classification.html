
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification model &#8212; OpenVINO Training Extensions  documentation</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Object Detection model" href="detection.html" />
    <link rel="prev" title="How to train, validate, export and optimize the model" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/otx-logo-black-mini.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../index.html">
   Base tutorial
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     How to train, validate, export and optimize the model
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Classification  model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="detection.html">
       Object Detection model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="instance_segmentation.html">
       Instance Segmentation model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="semantic_segmentation.html">
       Semantic Segmentation model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="anomaly_detection.html">
       Anomaly Classification model
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deploy.html">
     How to deploy the model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../demo.html">
     How to run the demo with exportable code
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../advanced/index.html">
   Advanced tutorial
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced/semi_sl.html">
     Use Semi-Supervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced/backbones.html">
     Backbone Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced/datumaro.html">
     Convert a custom user dataset with Datumaro
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup-virtual-environment">
   Setup virtual environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-preparation">
   Dataset preparation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation">
   Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#export">
   Export
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="classification-model">
<h1>Classification  model<a class="headerlink" href="#classification-model" title="Permalink to this headline">Â¶</a></h1>
<p>This live example shows how to easily train, validate, optimize and export classification model on the <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/image_feature_vector#the_flowers_dataset">flowers dataset</a> from TensorFlow.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To learn deeper how to manage training process of the model including additional parameters and its modification, refer to <a class="reference internal" href="detection.html"><span class="doc">Object Detection model</span></a>.</p>
<p>To learn how to deploy the trained model, refer to: <a class="reference internal" href="../deploy.html"><span class="doc">How to deploy the model</span></a>.</p>
<p>To learn how to run the demo and visualize results, refer to: <a class="reference internal" href="../demo.html"><span class="doc">How to run the demo with exportable code</span></a>.</p>
</div>
<p>The process has been tested on the following configuration.</p>
<ul class="simple">
<li><p>Ubuntu 20.04</p></li>
<li><p>NVIDIA GeForce RTX 3090</p></li>
<li><p>Intel(R) Core(TM) i9-10980XE</p></li>
<li><p>CUDA Toolkit 11.1</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While this example shows how to work with <a class="reference internal" href="../../../explanation/algorithms/classification/multi_class_classification.html"><span class="doc">multi-class classification</span></a>, it is easy to extend it for the <a class="reference internal" href="../../../explanation/algorithms/classification/multi_label_classification.html"><span class="doc">multi-label</span></a> or <a class="reference internal" href="../../../explanation/algorithms/classification/hierarhical_classification.html"><span class="doc">hierarchical</span></a> classification.
Substitute the dataset with a multi-label or hierarchical one. Everything else remains the same.</p>
</div>
<div class="section" id="setup-virtual-environment">
<h2>Setup virtual environment<a class="headerlink" href="#setup-virtual-environment" title="Permalink to this headline">Â¶</a></h2>
<p>You can follow the installation process from a <a class="reference internal" href="../../../get_started/installation.html"><span class="doc">quick start guide</span></a> to create a universal virtual environment for OTX.</p>
</div>
<div class="section" id="dataset-preparation">
<h2>Dataset preparation<a class="headerlink" href="#dataset-preparation" title="Permalink to this headline">Â¶</a></h2>
<p>Download and prepare a <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/image_feature_vector#the_flowers_dataset">flowers dataset</a>
with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">data</span>
<span class="n">wget</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="o">.</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">example_images</span><span class="o">/</span><span class="n">flower_photos</span><span class="o">.</span><span class="n">tgz</span>
<span class="n">tar</span> <span class="o">-</span><span class="n">xzvf</span> <span class="n">flower_photos</span><span class="o">.</span><span class="n">tgz</span>
<span class="n">cd</span> <span class="o">..</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<a class="reference internal image-reference" href="../../../_images/flowers.jpg"><img alt="../../../_images/flowers.jpg" src="../../../_images/flowers.jpg" style="width: 600px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>This dataset contains images of 5 different flower categories and is stored in the imagenet format which is supported by OTX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>flower_photos
  âââ daisy
  âââ dandelion
  âââ roses
  âââ sunflowers
  âââ tulips
</pre></div>
</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">Â¶</a></h2>
<p>1. First of all, we need to choose which classification model will we train.
The list of supported templates for classification is available with the command line below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The characteristics and detailed comparison of the models could be found in <a class="reference internal" href="../../../explanation/algorithms/classification/multi_class_classification.html"><span class="doc">Explanation section</span></a>.</p>
<p>We also can modify the architecture of supported models with various backbones, please refer to the <a class="reference internal" href="../../advanced/backbones.html"><span class="doc">advanced tutorial for model customization</span></a>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx find --task classification

+----------------+---------------------------------------------------+-----------------------+-----------------------------------------------------------------------------------+
|      TASK      |                         ID                        |          NAME         |                                        PATH                                       |
+----------------+---------------------------------------------------+-----------------------+-----------------------------------------------------------------------------------+
| CLASSIFICATION | Custom_Image_Classification_MobileNet-V3-large-1x | MobileNet-V3-large-1x | otx/algorithms/classification/configs/mobilenet_v3_large_1_cls_incr/template.yaml |
| CLASSIFICATION |    Custom_Image_Classification_EfficinetNet-B0    |    EfficientNet-B0    |    otx/algorithms/classification/configs/efficientnet_b0_cls_incr/template.yaml   |
| CLASSIFICATION |   Custom_Image_Classification_EfficientNet-V2-S   |   EfficientNet-V2-S   |   otx/algorithms/classification/configs/efficientnet_v2_s_cls_incr/template.yaml  |
+----------------+---------------------------------------------------+-----------------------+-----------------------------------------------------------------------------------+
</pre></div>
</div>
<p>To have a specific example in this tutorial, all commands will be run on the <a class="reference internal" href="../../../explanation/algorithms/classification/multi_class_classification.html#classificaiton-models"><span class="std std-ref">MobileNet-V3-large-1x</span></a>  model. Itâs a light model, that achieves competitive accuracy while keeping the inference fast.</p>
<ol class="arabic simple" start="2">
<li><p>Next, we need to create train/validation sets. OTX supports auto-split functionality for the multi-class classificaiton. For other classification types we need to prepare splits in advance.</p></li>
</ol>
<p>Letâs prepare an OTX classification workspase running the following command:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, OTX supports auto-split only for multi-class classificaiton. For the multi-label and hierarchical tasks we need to prepare data splits in advance.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx build --train-data-roots data/flower_photos --model MobileNet-V3-large-1x

[*] Load Model Template ID: Custom_Image_Classification_MobileNet-V3-large-1x
[*] Load Model Name: MobileNet-V3-large-1x
[*] Saving data configuration file to: ./otx-workspace-CLASSIFICATION-MobileNet-V3-large-1x/data.yaml

(otx) ...$ cd ./otx-workspace-CLASSIFICATION-MobileNet-V3-large-1x
</pre></div>
</div>
<p>It will create <strong>otx-workspace-CLASSIFICATION</strong> with all necessery configs for MobileNet-V3-large-1x, prepared <code class="docutils literal notranslate"><span class="pre">data.yaml</span></code> to simplify CLI commands launch and splitted dataset.</p>
<p>2. To start training we need to call <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">train</span></code>
command in our worspace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx train
</pre></div>
</div>
<p>Thatâs it! The training will return artifacts: <code class="docutils literal notranslate"><span class="pre">weights.pth</span></code> and <code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code>, which are needed as input for the further commands: <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">eval</span></code>,  <code class="docutils literal notranslate"><span class="pre">optimize</span></code>,  etc.</p>
<p>The training time highly relies on the hardware characteristics, for example on 1 GeForce 3090 the training took about 8 minutes.</p>
<p>After that, we have the PyTorch classification model trained with OTX, which we can use for evaluation, export, optimization and deployment.</p>
</div>
<div class="section" id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Permalink to this headline">Â¶</a></h2>
<p>1. <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> runs evaluation of a trained
model on a specific dataset.</p>
<p>The eval function receives test annotation information and model snapshot, trained in the previous step.
Please note, <code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code> file contains meta-information about the dataset and it should be located in the same folder as the model snapshot.</p>
<p><code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> will output a top-1 accuracy score for multi-class classification.</p>
<p>2. The command below will run validation on our dataset
and save performance results in <code class="docutils literal notranslate"><span class="pre">performance.json</span></code> file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx eval --test-data-roots splitted_dataset/val \
                    --load-weights models/weights.pth \
                    --save-performance performance.json
</pre></div>
</div>
<p>We will get a similar to this validation output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>

<span class="mi">2023</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">03</span> <span class="mi">23</span><span class="p">:</span><span class="mi">43</span><span class="p">:</span><span class="mi">29</span><span class="p">,</span><span class="mi">514</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">run</span> <span class="n">task</span> <span class="n">done</span><span class="o">.</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">03</span> <span class="mi">23</span><span class="p">:</span><span class="mi">43</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">859</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">called</span> <span class="n">evaluate</span><span class="p">()</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">03</span> <span class="mi">23</span><span class="p">:</span><span class="mi">43</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">870</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Accuracy</span> <span class="n">after</span> <span class="n">evaluation</span><span class="p">:</span> <span class="mf">0.9659400544959128</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">03</span> <span class="mi">23</span><span class="p">:</span><span class="mi">43</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">871</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Evaluation</span> <span class="n">completed</span>
<span class="n">Performance</span><span class="p">(</span><span class="n">score</span><span class="p">:</span> <span class="mf">0.9659400544959128</span><span class="p">,</span> <span class="n">dashboard</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span> <span class="n">metric</span> <span class="n">groups</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="export">
<h2>Export<a class="headerlink" href="#export" title="Permalink to this headline">Â¶</a></h2>
<p>1. <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">export</span></code> exports a trained Pytorch <cite>.pth</cite> model to the OpenVINOâ¢ Intermediate Representation (IR) format.
It allows running the model on the Intel hardware much more efficient, especially on the CPU. Also, the resulting IR model is required to run POT optimization. IR model consists of 2 files: <code class="docutils literal notranslate"><span class="pre">openvino.xml</span></code> for weights and <code class="docutils literal notranslate"><span class="pre">openvino.bin</span></code> for architecture.</p>
<p>2. We can run the below command line to export the trained model
and save the exported model to the <code class="docutils literal notranslate"><span class="pre">openvino_model</span></code> folder.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx export --load-weights models/weights.pth \
                      --save-model-to openvino_model

...

2023-02-02 03:23:03,057 | INFO : run task done.
2023-02-02 03:23:03,064 | INFO : Exporting completed
</pre></div>
</div>
<p>3. We can check the accuracy of the IR model and the consistency between the exported model and the PyTorch model,
using <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> and passing the IR model path to the <code class="docutils literal notranslate"><span class="pre">--load-weights</span></code> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx eval --test-data-roots splitted_dataset/val \
                    --load-weights openvino_model/openvino.xml \
                    --save-performance openvino_model/performance.json

...

Performance(score: 0.9659400544959128, dashboard: (3 metric groups))
</pre></div>
</div>
</div>
<div class="section" id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">Â¶</a></h2>
<p>1. We can further optimize the model with <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">optimize</span></code>.
It uses NNCF or POT depending on the model format.</p>
<p>Please, refer to <a class="reference internal" href="../../../explanation/additional_features/models_optimization.html"><span class="doc">optimization explanation</span></a> section to get the intuition of what we use under the hood for optimization purposes.</p>
<p>2. Command example for optimizing
a PyTorch model (<cite>.pth</cite>) with OpenVINOâ¢ NNCF.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx optimize --load-weights models/weights.pth --save-model-to nncf_model

...

INFO:nncf:Loaded 983/983 parameters
2023-02-04 00:06:11,725 | INFO : run task done.
2023-02-04 00:06:16,924 | INFO : called evaluate()
2023-02-04 00:06:16,935 | INFO : Accuracy after evaluation: 0.9591280653950953
2023-02-04 00:06:16,936 | INFO : Evaluation completed
Performance(score: 0.9591280653950953, dashboard: (3 metric groups))
</pre></div>
</div>
<p>The optimization time relies on the hardware characteristics, for example on 1 GeForce 3090 and Intel(R) Core(TM) i9-10980XE it took about 10 minutes.</p>
<p>3.  Command example for optimizing
OpenVINOâ¢ model (.xml) with OpenVINOâ¢ POT.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx optimize --load-weights openvino_model/openvino.xml \
                        --save-model-to pot_model

...

Performance(score: 0.9577656675749319, dashboard: (3 metric groups))
</pre></div>
</div>
<p>Please note, that POT will take some time (generally less than NNCF optimization) without logging to optimize the model.</p>
<p>4. Now we have fully trained, optimized and exported an
efficient model representation ready-to-use classification model.</p>
<p>The following tutorials provide further steps on how to <a class="reference internal" href="../deploy.html"><span class="doc">deploy</span></a> and use your model in the <a class="reference internal" href="../demo.html"><span class="doc">demonstration mode</span></a> and visualize results.
The examples are provided with an object detection model, but it is easy to apply them for classification by substituting the object detection model with classification one.</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">How to train, validate, export and optimize the model</a>
    <a class='right-next' id="next-link" href="detection.html" title="next page">Object Detection model</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, OpenVINO Training Extensions Contributors.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>